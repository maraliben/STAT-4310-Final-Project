---
title: "Final Project RMD"
author: "Marali Benitez, Youcef Djoudi, Kelly Thackery, Samina Issa, Samira Bechi"
date: "2025-04-09"
output:
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
##loading in packages and data
library(wooldridge)
library(AER)
library(quantmod)
library(PerformanceAnalytics)
library(lmtest)

#ceosal2 data
ceosalary <- ceosal2

#Murder rates data
data("MurderRates")
mr <- MurderRates
```
#### Data: ceosal2

##### (a) Describe the data. Describe the variables; response, predictors, continuous, categorical variables and missing data.

```{r}
library(wooldridge)
data("ceosal2")
head(ceosal2)
```

**This dataset (`ceosal2`) includes information on CEOs and their companies. The analysis focuses on how various CEO and firm characteristics influence company profits**.

##### Variables:

**Response Variable:** - `profits`: Annual firm profits (in millions).\
- Type: **Continuous**.

| Variable   | Description                   | Type                 |
|------------|-------------------------------|----------------------|
| `salary`   | CEO salary (in thousands)     | Continuous           |
| `age`      | CEO age                       | Continuous           |
| `college`  | 1 = undergraduate degree      | Categorical (binary) |
| `grad`     | 1 = graduate degree           | Categorical (binary) |
| `comten`   | Years with the company        | Continuous           |
| `ceoten`   | Years as CEO                  | Continuous           |
| `sales`    | Company sales (in millions)   | Continuous           |
| `mktval`   | Market value (in millions)    | Continuous           |
| `lmktval`  | Log of market value           | Continuous           |
| `comtensq` | Square of company tenure      | Continuous           |
| `ceotensq` | Square of CEO tenure          | Continuous           |
| `profmarg` | Profit margin = profits/sales | Continuous           |

**Excluded Predictors:** - `lsalary`, `lsales`: Log-transformed versions of salary and sales; excluded from final model.

```{r}
#Check for missing values
colSums(is.na(ceosal2))
```

**NO missing values in the data set**.

##### (b) Build an optimal model. Print summary: discuss F-test, t-tests, Rsquare, S-square. Take out outliers.

```{r}
#preliminary model
model <- lm(profits ~ .-lsalary -lmktval -lsales, data=ceosal2)
summary(model)

```

**In the primary model, all variables were included except for lsalary, lmktval, and lsales. The RÂ² value is 0.877, indicating that approximately 87.7% of the variation in profits is accounted for by the model. The F-test p-value is less than 2.2e-16, confirming that the model is statistically significant and not due to random chance. Based on the summary output, the predictors that show statistical significance are sales, market value (mktval), and profit margin (profmarg). The residual standard error is 146.5, suggesting that, on average, the model's profit predictions differ from the actual values by about 146.5 units.**

```{r}
#taking out outliers
cooksd <- cooks.distance(model)
influential <- as.numeric(names(cooksd)[(cooksd > (4 / nrow(ceosalary)))])
influential
newceosalary <- ceosalary[-influential]
#creating reduced model with no outliers in model
newceosalary_model <- lm(profits~.-lsalary -lmktval -lsales, data = newceosalary)
summary(newceosalary_model)
#creating newer updated model using step function
updatedceosalmodel <- step(newceosalary_model, direction = "both", trace = 0)
summary(updatedceosalmodel)

```

**After calculating the cooks distance we found that there were no outliers according to cooks distance, so no observations were removed. The r^2 value has stayed relatively the same, but the model was reduced to only three predictor variables: sales, mktval, and profmarg. All of these variables, including the intercept, are statistically significant according to the p-value from the t-test. The f-test for this reduced model shows that it is also statistically significant**.  


##### (c) Briefly discuss residuals.

```{r}
# Q-Q plot: check for normality of residuals
qqnorm(residuals(updatedceosalmodel))
qqline(residuals(updatedceosalmodel), col = "red", lwd = 2)

```
**The normal qqplot deviates quite a bit from the normality line, suggesting that the residuals are not normally distributed**.  

```{r}
#checking if residuals are randomly distributed
plot(residuals(updatedceosalmodel))
abline(a = 0, b = 0)
```
**The residuals for this model seem to be randomly scattered around 0**.    


```{r}
#residuals vs fitted values plot
plot(updatedceosalmodel$fitted.values, residuals(updatedceosalmodel))
abline(a = 0, b = 0)
bptest(updatedceosalmodel)
```
**There does seem to somewhat of a pattern in the residuals vs fitted values plot, suggesting that there is heteroscedasticity present. This is confirmed by the Breusch-Pagan test (which has a p-value of about 6.818e-09), meaning that some predictinos made by the model may not be accurate.**

##### (d) Make 2 predictions with CI's. Interpret predictions if needed.

```{r}
# Hypothetical CEO 1
new_ceo1 <- data.frame(
  sales = 4500,
  mktval = 1800,
  profmarg = 10
)

# Hypothetical CEO 2
new_ceo2 <- data.frame(
  sales = 7000,
  mktval = 3000,
  profmarg = 15
)

#make predictions with 95% CI
# Predictions for CEO 1
predict(updatedceosalmodel, newdata = new_ceo1, interval = "confidence")

# Predictions for CEO 2
predict(updatedceosalmodel, newdata = new_ceo2, interval = "confidence")

```
**The first prediction is for a company with 4500 million dollars in profits, 1800 million in market value, and 10% profit margin. The second prediction is for a slightly more successful company with 7000 million in profits, 3000 million in market value, and a 15% profit margin**.   


#### Data: MurderRates.

##### (a) Describe the data. Describe the variables; response, predictors, continuous, categorical variables and missing data.

```{r}
head(mr)
```

**The murder rates data set contains data collected in 1950 that reflects 44 states' murder rates among other variables. The variables for this dataset are as follows: **

* rate (continuous): the states murder rate per 100,000 according to the FBI estimate. 
* convictions (continuous): the number of convictions divided by the number of murders in 1950.  
* executions (continuous): the average number of executions from 1946-1950 divided by the number of convictions in 1950.    
* time (discrete): median time (served in months) of convicted murderers in the state released in 1951.     
* income (continuous): median family income for the state in 1949 (in thousands of US dollars).    
* lfp (continuous): the labor force participation rate in the state in 1949.   
* noncaucasian (continous): proportion of the population that is non-cacasian in the state as of 1950.   
* southern (factor): indicates if the state is in the south or not.   

**There were no missing values in this dataset.**


##### (b) Build an optimal model. Print summary: discuss F-test, t-tests, Rsquare, S-square. Take out outliers**.

```{r}
#creating preliminary model
murdermodel <- lm(rate~., data = mr)
summary(murdermodel)

#calculation for preliminary models' s-squared value
#mean(murdermodel$residuals^2)
```

*In the preliminary model, all predictor variables were used to create the model. The R\^2 value for this preliminary model is .7459, meaning that 74.59% of the variation in the response can be explained by these predictor variables. The p-value for the f-test in this preliminary model is 5.105e-09, meaning that this model is non trivial. The summary output for the preliminary model also shows that the only statistically significant predictors in this model are time and the southern factor. The S-squared value for this model is about 4.94, meaning that on average, the preliminary model's predictions deviate by about 4.94 murders from the true murder rate.*

```{r}
#taking out outliers
cooks <- cooks.distance(murdermodel)
inf <- as.numeric(names(cooks)[cooks>(4/44)])
newmurderdata <- mr[-inf,]

#creating reduced model with no outliers in model
newmurdermodel <- lm(rate~., data = newmurderdata)
summary(newmurdermodel)

#creating newer updated model using step function
updatedmurdermodel <- step(newmurdermodel, direction = "both", trace = 0)
summary(updatedmurdermodel)
```

*Removing the outliers took out about 9% of the data. After taking out the outliers, we can see that the R\^2 value has increased to .8288, and the the S\^2 value has gone down to 1.841. We then reduced the model using the step function, where we can see that the r\^2 and s\^2 did not change by much (the new r\^2 value is .8234 and the new s\^2 value is 1.814). The predictors for this model have been reduced to convictions, time, income, noncaucasian, and southern. More of the predictors are statistically significant in this model, with the intercept, convictions, noncaucasian, and southern all being statistically significant according to the p-value found by the t-tests. The f-test for this reduced model shows that it is nontrivial, as the p-value for the f-test is 7.059e-12.*

##### (c) Briefly discuss residuals.

```{r}
##taking a look at residuals

#checking if data is normally distributed
qqnorm(residuals(updatedmurdermodel))
qqline(residuals(updatedmurdermodel))
```

*The normal qqplot for this model suggests that the residuals in this model are normally distributed*.

```{r}
#checking if residuals are randomly distributed
plot(residuals(updatedmurdermodel))
abline(a = 0, b = 0)
```

*The residuals for this model also seem to be randomly scattered around 0, meaning that a linear model may be a good fit for the data*.

```{r}
#residuals vs fitted values plot
plot(updatedmurdermodel$fitted.values, residuals(updatedmurdermodel))
abline(a = 0, b = 0)
bptest(updatedmurdermodel)
```

*In the fitted values vs residuals plot, there does seem to somewhat of a pattern, suggesting that there is heteroscedasticity present. This is confirmed by the Breusch-Pagan test (which has a p-value of about .038)*.

##### (d) Make 2 predictions with CI's. Interpret predictions if needed.

```{r}
## hypothetical state 1
state1 <- data.frame(
  convictions = .3,
  time = 150,
  income = 2,
  noncauc = .015,
  southern = "yes"
)

##hypothetical state 2
state2 <- data.frame(
  convictions = .15,
  time =100,
  income = 3,
  noncauc = .2,
  southern = "no"
)

prediction1 <- predict(updatedmurdermodel, newdata = state1, interval = "confidence")
prediction2 <- predict(updatedmurdermodel, newdata = state2, interval = "confidence")

prediction1
prediction2
```

*Hypothetical state 1 inputs: convictions = .3, time = 150, income = 2, noncaucasian = .015, southern = "yes". Hypothetical state 2 inputs: convictions = .15, time = 100, income = 3, noncaucasian = .2, southern = "no". The 95% confidence interval for hypothetical state 1 is (4.46, 8.6), meaning that for a state with those inputs, we are 95% confident that the true murder rate per 100,000 per the FBI estimates for that state is in between those two values. For the 2nd hypothetical state, we would be 95% that the true murder rate per 100,000 per the FBI estimates for that state is in between (.43, 6.42).*

#### Finance Application

##### Read stock prices for 5 different stocks. Close prices for 3 years, most recent 2024. Criteria Beta \> 1, PE \> 10, growth \> 10%, different industries.  
 

```{r}
# Read stock prices
start<- as.Date("2021-01-01")
end<-as.Date("2024-01-01")
DIS <- getSymbols("DIS", from=start, to=end, auto.assign=FALSE)
NVDA <- getSymbols("NVDA", from=start, to=end, auto.assign=FALSE)
CVNA<-getSymbols("CVNA", from=start, to=end, auto.assign=FALSE)
SPOT<-getSymbols("SPOT", from=start, to=end, auto.assign=FALSE)
DASH<-getSymbols("DASH", from=start, to=end, auto.assign=FALSE)


## Closed price
DIS.close <- DIS[ ,4]
NVDA.close <- NVDA[ ,4]
CVNA.close <- CVNA[ ,4]
SPOT.close <- SPOT[ ,4]
DASH.close <- DASH[ ,4]

## Returns
DIS.return <- dailyReturn(DIS.close)
NVDA.return <- dailyReturn(NVDA.close)
CVNA.return <- dailyReturn(CVNA.close)
SPOT.return <- dailyReturn(SPOT.close)
DASH.return <- dailyReturn(DASH.close)
```

**The stocks selected are: Disney, Nvidia, Carvana, Spotify, and DoorDash. The criteria for selection were:\  
Disney: beta(1.44), P/E(29.69), Growth Estimate(11.28%), Industry: Entertainment\  
Nvidia: beta(1.96), P/E(38.89), Growth Estimate(27.98%), Industry: Semiconductors\  
Carvana: beta(3.62), P/E(138.64), Growth Estimate(43.53%), Industry: Auto & Truck Dealerships\   Spotify: beta(1.75), P/E(90.03), Growth Estimate(27.20%), Industry: Internet Content & Information\  
DoorDash: beta(1.69), P/E(627.07), Growth Estimate(31.84%), Industry: Communication Services**   

##### (a)Plot close prices on three different plots. Notice the general "trend" and "variation" on different spans.

```{r, warning=FALSE}
#Plot close stock prices on different plots.
plot(DIS.close, main = "Disney Closing Prices", type = "l", col = "blue", xlab = "Date", ylab = "Price")
plot(NVDA.close, main = "NVIDIA Closing Prices", type = "l", col = "red", xlab = "Date", ylab = "Price")
plot(CVNA.close, main = "Carvana Closing Prices", type = "l", col = "green", xlab = "Date", ylab = "Price")
plot(SPOT.close, main = "Spotify Closing Prices", type = "l", col = "magenta", xlab = "Date", ylab = "Price")
plot(DASH.close, main = "DoorDash Closing Prices", type = "l", col = "purple", xlab = "Date", ylab = "Price")
```

**Remark: The closing prices for Disney, Carvana, Spotify and Doordash show a decline in 2022. Carvana doesn't have much variation through 2023. Spotify shows an increase with slight variation in 2023. Disney and Doordash both show variation in 2023. The closing price for NVIDIA rises until the end of 2021 then falls until it starts to rise again toward the end of 2022.**

##### (b) Calculate annualized average return and annualized risk. Present the correlations matrix.

```{r, warning=FALSE}
library(PerformanceAnalytics)
#Calculate the annualized return and annualized risk of each stock.
## A function to compute the Annualized Expected return/Risk
mu.sigma <- function(return){
  mu.ann <- mean(return) * 252
  sigma.ann <- sd(return) * sqrt(252)
  return(c(mu.ann, sigma.ann))
}

## Annualized Expected Return and Annualized Risk
dis <- mu.sigma(DIS.return)
nvda <- mu.sigma(NVDA.return)
cvna <- mu.sigma(CVNA.return)
spot <- mu.sigma(SPOT.return)
dash <- mu.sigma(DASH.return)


cat('Disney:', dis, '\n')
cat('NVIDIA:', nvda, '\n')
cat('Carvana:', cvna, '\n')
cat('Spoify:', spot, '\n')
cat('DoorDash:', dash, '\n')


## Correlation Matrix
returns <- cbind(DIS.return,NVDA.return,CVNA.return,SPOT.return,DASH.return)
colnames(returns) <- c('Disney', 'NVIDIA', 'Carvana', 'Spotify','DoorDash')
head(returns)
chart.Correlation(returns)
```

**Remark: Disney has a -18.1% return with a 29.9% risk.\
NVIDIA has a 58.3% return with a 53% risk.\
Carvana has a 31.3% return with a 129.6% risk.\
Spoify has a -3.4% return with a 51.7% risk.\
DoorDash has a 10.9% return with a 67.1% risk. From the correlation coefficients we can see these companies generally move in the same direction. The Spotify-Doordash scatter plot shows a tight upward trend, indicating a strong positive correlation between the two.**

##### (c)Plot cumulative returns on one common plot.

```{r, warning=FALSE}
## SP500(Benchmark)
SP500<- getSymbols("^GSPC", from=start, to=end, auto.assign = FALSE)
SP500.close <- SP500[ , 5]
SP500.return<- dailyReturn(SP500.close)

## Combine all returns into one data frame
returns <- cbind(DIS.return, NVDA.return, CVNA.return, SPOT.return, DASH.return, SP500.return)
colnames(returns) <- c('Disney', 'NVIDIA', 'Carvana', 'Spotify', 'DoorDash', 'SP500')


## Plot cumulative returns using chart.CumReturns from PerformanceAnalytics
library(PerformanceAnalytics)
chart.CumReturns(returns, wealth.index = TRUE, legend.loc = 'topleft',
                 main = 'Cumulative Returns', colorset=c('blue','red','green','magenta','purple','black'))

```

**Remark: NVIDIA shows a significant upward trend, indicating strong growth. Disney, Carvana, Spotify, and DoorDash have more moderate growth.**

##### (d)Estimate alpha, beta, Rsquare.

```{r}

##Riskfree rates 
rf <- read.csv("F-F_Research_Data_Factors_daily.CSV", head=T, skip=3) # a data frame
head(rf)
rf$dates <- as.Date(rf$X, format="%Y%m%d") # create dates as X in the fama.french
## sort ff according to dates 
rf.new <- rf[rf$dates >= "2021-01-01" & rf$dates<= "2024-01-01", ] 
head(rf.new)
rf.new <- rf.new[ -754, ]


# Estimate alpha, beta, Rsquare of 3 stocks.
beta <- function(stock.return, market.return, riskfree.rate) {
    stock.excess <- stock.return - riskfree.rate
    market.excess <- market.return - riskfree.rate
    model <- lm(stock.excess ~ market.excess)
    coefs <- coef(model)
    Rsquare <- summary(model)$r.squared
    results <- data.frame(alpha = coefs[1], beta = coefs[2], Rsquare = Rsquare)
    return(results)
}

# Call beta function for each stock
res_DIS   <- beta(DIS.return, SP500.return, rf.new$RF)
res_NVDA  <- beta(NVDA.return, SP500.return, rf.new$RF)
res_CVNA  <- beta(CVNA.return, SP500.return, rf.new$RF)
res_SPOT  <- beta(SPOT.return, SP500.return, rf.new$RF)
res_DASH  <- beta(DASH.return, SP500.return, rf.new$RF)

# Combine the individual results into one table
results_table <- rbind(Disney  = res_DIS, NVIDIA  = res_NVDA, Carvana = res_CVNA, SPOT = res_SPOT, DASH = res_DASH)

# Display the resulting table
print(results_table)
```

**Remark: The alphas tell us the stocks underperformed compared to what the market model expected, the returns didn't justify the risk. Betas are less than the baseline of 1, suggesting less sensitivity to market wide fluctuations. A low Rsquared value stock's return variability is not market dependent.**  
