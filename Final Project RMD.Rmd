---
title: "Final Project RMD"
author: "Marali Benitez, Youcef Djoudi, Kelly Thackery, Samina Issa, Samira Bechi"
date: "2025-04-09"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE}
##loading in packages and data
library(wooldridge)
library(AER)
library(quantmod)
library(PerformanceAnalytics)

#ceosal2 data
ceosalary <- ceosal2

#Murder rates data
data("MurderRates")
mr <- MurderRates
```

**Data: ceosal2**

**(a) Describe the data. Describe the variables; response, predictors, continuous, categorical variables and missing data**.   



**(b) Build an optimal model. Print summary: discuss F-test, t-tests, Rsquare, S-square. Take out outliers**.  
```{r}
model<- lm(profits ~. -lsalary -lsales, ceosalary)
summary(model)

```
**(c) Briefly discuss residuals**.  



**(d) Make 2 predictions with CI's. Interpret predictions if needed**.  




**Data: CPS 1988**.   

**(a) Describe the data. Describe the variables; response, predictors, continuous, categorical variables and missing data**.       

```{r}
head(mr)
```

*The murder rates data set contains data collected in 1950 that reflects 44 states' murder rate per 100,000 according to the FBI estimate, the convictions (which in this data set reflects the number of convictions divided by number of murders in 1950), the average number of executions from 1946-1950 divided by the number of convictions in 1950, the median time served in months of convicted murderers released in 1951, the median family income for the state in 1949 in thousands of dollars, the labor force participation rate for the state in 1950 (in percent), the proportion of the states population that is non-Caucasian in 1950, and southern, which indicates if the state is in the south or not. The response variable in this data would be the rate, while the other variables would be the predictors. There were no missing values in this data. *       



**(b) Build an optimal model. Print summary: discuss F-test, t-tests, Rsquare, S-square. Take out outliers**.      

```{r}
#creating preliminary model
murdermodel <- lm(rate~., data = mr)
summary(murdermodel)

#calculation for preliminary models' s-squared value
#mean(murdermodel$residuals^2)
```

*In the preliminary model, all predictor variables were used to create the model. The R^2 value for this preliminary model is .7459, meaning that 74.59% of the variation in the response can be explained by these predictor variables. The p-value for the f-test in this preliminary model is 5.105e-09, meaning that this model is non trivial. The summary output for the preliminary model also shows that the only statistically significant predictors in this model are time and the southern factor. The S-squared value for this model is about 4.94, meaning that on average, the preliminary model's predictions deviate by about 4.94 murders from the true murder rate.*        


```{r}
#taking out outliers
cooks <- cooks.distance(murdermodel)
inf <- as.numeric(names(cooks)[cooks>(4/44)])
newmurderdata <- mr[-inf,]

#creating reduced model with no outliers in model
newmurdermodel <- lm(rate~., data = newmurderdata)
summary(newmurdermodel)

#creating newer updated model using step function
updatedmurdermodel <- step(newmurdermodel, direction = "both", trace = 0)
summary(updatedmurdermodel)
```
*After taking out the outliers, we can see that the R^2 value has increased to .8288, and the the S^2 value has gone down to 1.841. We then reduced the model using the step function, where we can see that the r^2 and s^2 did not change by much (the new r^2 value is .8234 and the new s^2 value is 1.814). The predictors for this model have been reduced to convictions, time, income, noncaucasian, and southern. More of the predictors are statistically significant in this model, with the intercept, convictions, noncaucasian, and southern all being statistically significant according to the p-value found by the t-tests. The f-test for this reduced model shows that it is nontrivial, as the p-value for the f-test is 7.059e-12.*    



**(c) Briefly discuss residuals**.  



**(d) Make 2 predictions with CI's. Interpret predictions if needed**.  

## Finance Application  

**Read stock prices for 5 different stocks. Close prices for 3 years, most recent 2024. Criteria Beta > 1, PE > 10, growth > 10%, different industries.**
```{r}
# Read stock prices
start<- as.Date("2021-01-01")
end<-as.Date("2024-01-01")
DIS <- getSymbols("DIS", from=start, to=end, auto.assign=FALSE)
NVDA <- getSymbols("NVDA", from=start, to=end, auto.assign=FALSE)
CVNA<-getSymbols("CVNA", from=start, to=end, auto.assign=FALSE)
SPOT<-getSymbols("SPOT", from=start, to=end, auto.assign=FALSE)
DASH<-getSymbols("DASH", from=start, to=end, auto.assign=FALSE)


## Closed price
DIS.close <- DIS[ ,3]
NVDA.close <- NVDA[ ,3]
CVNA.close <- CVNA[ ,3]
SPOT.close <- SPOT[ ,3]
DASH.close <- SPOT[ ,3]

## Returns
DIS.return <- dailyReturn(DIS.close)
NVDA.return <- dailyReturn(NVDA.close)
CVNA.return <- dailyReturn(CVNA.close)
SPOT.return <- dailyReturn(SPOT.close)
DASH.return <- dailyReturn(DASH.close)


```

**Disney: beta(1.44), P/E(29.69), Growth Estimate(29.1), Industry: Entertainment    
  Nvidia: beta(1.96), P/E(38.89), Growth Estimate(13.7%), Industry: Semiconductors    
  Carvana: beta(3.62), P/E(138.64), Growth Estimate(65.9%), Industry: Auto & Truck Dealerships     Spotify: beta(1.75), P/E(90.03), Growth Estimate(23.1%), Industry: Internet Content & Information  
  DoorDash: beta(1.69), P/E(627.07), Growth Estimate(36.8%), Industry: Communication Services**      
        
**(a)Plot close prices on three different plots. Notice the general "t????" "variation" on different spans.**  
```{r, warning=FALSE}
#Plot close stock prices on different plots.
plot(DIS.close, main = "Disney Closing Prices", type = "l", col = "blue", xlab = "Date", ylab = "Price")
plot(NVDA.close, main = "NVIDIA Closing Prices", type = "l", col = "red", xlab = "Date", ylab = "Price")
plot(CVNA.close, main = "Carvana Closing Prices", type = "l", col = "green", xlab = "Date", ylab = "Price")
plot(SPOT.close, main = "Spotify Closing Prices", type = "l", col = "magenta", xlab = "Date", ylab = "Price")
plot(DASH.close, main = "DoorDash Closing Prices", type = "l", col = "purple", xlab = "Date", ylab = "Price")
```

**Remark: There is significant drop for Disney early in 2022 and then remains volatile. NVidia has seen a steady rise witha few minor dips in last half of 2024. We see Carvana fall spring 2022 and doesn't start to rise again until spring 2024. **  

**(b)Calculate annualized average return and annualized risk. Present the correlations matrix.**  
```{r, warning=FALSE}
library(PerformanceAnalytics)
#Calculate the annualized return and annualized risk of each stock.
## A function to compute the Annualized Expected return/Risk
mu.sigma <- function(return){
  mu.ann <- mean(return) * 252
  sigma.ann <- sd(return) * sqrt(252)
  return(c(mu.ann, sigma.ann))
}

## Annualized Expected Return and Annualized Risk
dis <- mu.sigma(DIS.return)
nvda <- mu.sigma(NVDA.return)
cvna <- mu.sigma(CVNA.return)
spot <- mu.sigma(SPOT.return)
dash <- mu.sigma(DASH.return)


cat('Disney:', dis, '\n')
cat('NVIDIA:', nvda, '\n')
cat('Carvana:', cvna, '\n')
cat('Spoify:', spot, '\n')
cat('DoorDash:', dash, '\n')


## Correlation Matrix
returns <- cbind(DIS.return,NVDA.return,CVNA.return,SPOT.return,DASH.return)
colnames(returns) <- c('Disney', 'NVIDIA', 'Carvana', 'Spotify','DoorDash')
head(returns)
chart.Correlation(returns)
```


**Remark: **  

**(c)Plot cumulative returns on one common plot.**  
```{r, warning=FALSE}
## Compute cumulative returns
cumulative_returns <- cumprod(1 + returns)

## Using chart.CumReturns from PerformanceAnalytics
chart.CumReturns(returns, wealth.index = TRUE, legend.loc = 'topleft',
                 main = 'Cumulative Returns', colorset = c('blue', 'red','green','magenta','purple'))

```

**Remark: **  

**(d)Estimate alpha, beta, Rsquare.**  
```{r}
## SP500(Benchmark)
SP500<- getSymbols("^GSPC", from=start, to=end, auto.assign = FALSE)
SP500.close <- SP500[ , 5]
SP500.return<- dailyReturn(SP500.close)

##Riskfree rates 
rf <- read.csv(file.choose(), head=T, skip=3) # a data frame
head(rf)
rf$dates <- as.Date(rf$X, format="%Y%m%d") # create dates as X in the fama.french
## sort ff according to dates 
rf.new <- rf[rf$dates >= "2021-01-01" & rf$dates<= "2024-01-01", ] 
head(rf.new)
#dim(rf.new)
#tail(rf.new)
rf.new <- rf.new[ -754, ]
#dim(rf.new)

# Estimate alpha, beta, Rsquare of 3 stocks.
beta <- function(stock.return, market.return, riskfree.rate) {
    stock.excess <- stock.return - riskfree.rate
    market.excess <- market.return - riskfree.rate
    model <- lm(stock.excess ~ market.excess)
    coefs <- coef(model)
    Rsquare <- summary(model)$r.squared
    results <- data.frame(alpha = coefs[1], beta = coefs[2], Rsquare = Rsquare)
    return(results)
}

# Call beta function for each stock
res_DIS   <- beta(DIS.return, SP500.return, rf.new$RF)
res_NVDA  <- beta(NVDA.return, SP500.return, rf.new$RF)
res_CVNA  <- beta(CVNA.return, SP500.return, rf.new$RF)
res_SPOT  <- beta(SPOT.return, SP500.return, rf.new$RF)
res_DASH  <- beta(DASH.return, SP500.return, rf.new$RF)

# Combine the individual results into one table
results_table <- rbind(Disney  = res_DIS, NVIDIA  = res_NVDA, Carvana = res_CVNA, 
                       SPOT = res_SPOT, DASH = res_DASH)

# Display the resulting table
print(results_table)

```

**Remark: **  





